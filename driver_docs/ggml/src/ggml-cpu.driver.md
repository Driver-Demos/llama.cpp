## Folders
- **[amx](ggml-cpu/amx.driver.md)**: The `amx` folder in the `llama.cpp` codebase contains source and header files that implement and declare functions for AMX-specific tensor operations, buffer management, and matrix multiplication, with support for various quantized data types and hardware configurations.
- **[cmake](ggml-cpu/cmake.driver.md)**: The `cmake` folder in the `llama.cpp` codebase contains configuration files for detecting and setting up SIMD instruction sets during the build process.
- **[kleidiai](ggml-cpu/kleidiai.driver.md)**: The `kleidiai` folder in the `llama.cpp` codebase contains files that implement and define CPU-specific optimizations and kernel operations for matrix multiplication and vector operations, focusing on the Kleidiai backend and ARM CPU features.
- **[llamafile](ggml-cpu/llamafile.driver.md)**: The `llamafile` folder in the `llama.cpp` codebase contains files related to implementing and declaring a multithreaded CPU matrix multiplication function for single-precision general matrix operations.

## Files
- **[binary-ops.cpp](ggml-cpu/binary-ops.cpp.driver.md)**: The `binary-ops.cpp` file in the `llama.cpp` codebase implements various binary operations such as addition, subtraction, multiplication, and division for tensors, with support for different data types and optional use of the Accelerate framework for optimization.
- **[binary-ops.h](ggml-cpu/binary-ops.h.driver.md)**: The `binary-ops.h` file declares functions for performing forward binary operations such as addition, subtraction, multiplication, and division on tensors in the `llama.cpp` codebase.
- **[CMakeLists.txt](ggml-cpu/CMakeLists.txt.driver.md)**: The `CMakeLists.txt` file in the `llama.cpp/ggml/src/ggml-cpu` directory defines a function to configure and add CPU backend variants for the GGML library, including setting source files, compile features, and architecture-specific flags and libraries.
- **[common.h](ggml-cpu/common.h.driver.md)**: The `common.h` file in the `llama.cpp` codebase provides utility functions and type conversion templates for handling different floating-point formats and thread range calculations in the `ggml` library.
- **[cpu-feats-x86.cpp](ggml-cpu/cpu-feats-x86.cpp.driver.md)**: The `cpu-feats-x86.cpp` file in the `llama.cpp` codebase implements a mechanism to detect and score various x86 CPU features and capabilities, such as SSE, AVX, and AVX512, using the CPUID instruction.
- **[ggml-cpu-aarch64.cpp](ggml-cpu/ggml-cpu-aarch64.cpp.driver.md)**: The `ggml-cpu-aarch64.cpp` file in the `llama.cpp` codebase is a specialized C++ implementation focused on optimized quantized matrix operations for machine learning, utilizing SIMD instructions like AVX2, AVX512, NEON, and SVE to enhance performance across various CPU architectures, and includes functions and data structures for handling quantized data, matrix multiplication, and tensor repacking, specifically tailored for ARM AArch64 architecture.
- **[ggml-cpu-aarch64.h](ggml-cpu/ggml-cpu-aarch64.h.driver.md)**: The `ggml-cpu-aarch64.h` file is an internal header in the `llama.cpp` codebase that declares a function for determining the buffer type for the AArch64 CPU backend.
- **[ggml-cpu-hbm.cpp](ggml-cpu/ggml-cpu-hbm.cpp.driver.md)**: The `ggml-cpu-hbm.cpp` file in the `llama.cpp` codebase provides functions for managing high-bandwidth memory (HBM) buffers on the CPU, including allocation and deallocation, when the `GGML_USE_CPU_HBM` flag is defined.
- **[ggml-cpu-hbm.h](ggml-cpu/ggml-cpu-hbm.h.driver.md)**: The `ggml-cpu-hbm.h` file in the `llama.cpp` codebase is an internal header for the GGML CPU that declares a function to get the buffer type for CPU HBM.
- **[ggml-cpu-impl.h](ggml-cpu/ggml-cpu-impl.h.driver.md)**: The `ggml-cpu-impl.h` file in the `llama.cpp` codebase provides internal CPU-specific implementations and optimizations for various vector operations, supporting multiple architectures and instruction sets.
- **[ggml-cpu-quants.c](ggml-cpu/ggml-cpu-quants.c.driver.md)**: The `ggml-cpu-quants.c` file in the `llama.cpp` codebase implements a highly optimized set of functions for quantization and vector dot product operations, utilizing SIMD instructions across various hardware architectures to enhance performance in machine learning and signal processing applications, and includes functions for quantizing data into different bit-width formats, computing dot products between quantized vectors, and handling quantized data efficiently.
- **[ggml-cpu-quants.h](ggml-cpu/ggml-cpu-quants.h.driver.md)**: The `ggml-cpu-quants.h` file in the `llama.cpp` codebase provides internal declarations for quantization and dot product operations on CPU, supporting various quantization levels and formats.
- **[ggml-cpu-traits.cpp](ggml-cpu/ggml-cpu-traits.cpp.driver.md)**: The `ggml-cpu-traits.cpp` file in the `llama.cpp` codebase implements functions for computing forward operations and determining work size for tensors using extra buffer types in a CPU backend.
- **[ggml-cpu-traits.h](ggml-cpu/ggml-cpu-traits.h.driver.md)**: The `ggml-cpu-traits.h` file defines interfaces and functions for handling extra computation traits and buffer types for CPU operations within the `llama.cpp` codebase.
- **[ggml-cpu.c](ggml-cpu/ggml-cpu.c.driver.md)**: The `ggml-cpu.c` file in the `llama.cpp` codebase provides a comprehensive implementation for CPU-based operations, including threading, NUMA support, and various mathematical operations on tensors, with support for different CPU architectures and optimizations.
- **[ggml-cpu.cpp](ggml-cpu/ggml-cpu.c.driver.mdpp)**: The `ggml-cpu.cpp` file in the `llama.cpp` codebase implements the CPU backend for the GGML library, providing functions for initializing, managing, and computing graph plans on CPU devices, as well as handling CPU-specific features and configurations.
- **[ops.cpp](ggml-cpu/ops.cpp.driver.md)**: The `ops.cpp` file in the `llama.cpp` codebase is a comprehensive C++ library that implements a wide range of tensor operations essential for machine learning and neural network computations, including duplication, addition, scaling, convolution, pooling, normalization, flash attention, and RWKV, with support for various data types and optimized for high-performance computing with multi-threading capabilities.
- **[ops.h](ggml-cpu/ops.h.driver.md)**: The `ops.h` file in the `llama.cpp` codebase defines a series of function declarations for various tensor operations and computations, including arithmetic, normalization, pooling, and attention mechanisms, while also setting cache line size configurations.
- **[simd-mappings.h](ggml-cpu/simd-mappings.h.driver.md)**: The `simd-mappings.h` file in the `llama.cpp` codebase defines a set of C macros for SIMD (Single Instruction, Multiple Data) operations, mapping them to specific intrinsics based on the current architecture to facilitate fundamental computation operations.
- **[unary-ops.cpp](ggml-cpu/unary-ops.cpp.driver.md)**: The `unary-ops.cpp` file in the `llama.cpp` codebase implements various unary operations such as absolute value, sign, negation, step function, and others, and applies them to tensors using a template-based approach for different data types.
- **[unary-ops.h](ggml-cpu/unary-ops.h.driver.md)**: The `unary-ops.h` file in the `llama.cpp` codebase declares functions for performing various unary operations on tensors, such as absolute value, sign, negation, and several activation functions.
- **[vec.cpp](ggml-cpu/vec.cpp.driver.md)**: The `vec.cpp` file in the `llama.cpp` codebase implements various vector operations, including dot products for different data types (f32, bf16, f16), the SiLU activation function, and softmax and log softmax functions, with optimizations for SIMD architectures.
- **[vec.h](ggml-cpu/vec.h.driver.md)**: The `vec.h` file in the `llama.cpp` codebase provides vectorized functions for fundamental operations, including dot products, element-wise arithmetic, and activation functions, with support for various data types and SIMD optimizations.
