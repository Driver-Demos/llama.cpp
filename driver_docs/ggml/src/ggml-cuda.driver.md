## Folders
- **[template-instances](ggml-cuda/template-instances.driver.md)**: The `template-instances` folder in the `llama.cpp` codebase contains autogenerated CUDA source files that declare specific instances of function templates for matrix multiplication and vector attention operations using various quantization types and configurations.
- **[vendors](ggml-cuda/vendors.driver.md)**: The `vendors` folder in the `llama.cpp` codebase contains header files that provide compatibility and integration support for various GPU computing platforms, including CUDA, HIP, and MUSA.

## Files
- **[acc.cu](ggml-cuda/acc.cu.driver.md)**: The `acc.cu` file in the `llama.cpp` codebase implements a CUDA kernel and associated functions for performing element-wise accumulation of two float arrays, with support for multi-dimensional indexing and offset handling.
- **[acc.cuh](ggml-cuda/acc.cu.driver.mdh)**: The `acc.cuh` file in the `llama.cpp` codebase defines a CUDA operation for accumulating data into a tensor using a specified block size.
- **[arange.cu](ggml-cuda/arange.cu.driver.md)**: The `arange.cu` file in the `llama.cpp` codebase implements a CUDA-based function to populate a tensor with a sequence of floating-point numbers, starting from a specified value and incrementing by a given step.
- **[arange.cuh](ggml-cuda/arange.cu.driver.mdh)**: The `arange.cuh` file in the `llama.cpp` codebase defines a CUDA operation for generating a range of values into a tensor using the `ggml_cuda_op_arange` function.
- **[argmax.cu](ggml-cuda/argmax.cu.driver.md)**: The `argmax.cu` file in the `llama.cpp` codebase implements a CUDA kernel to compute the index of the maximum value in each row of a 2D float tensor, storing the results in an integer tensor.
- **[argmax.cuh](ggml-cuda/argmax.cu.driver.mdh)**: The `argmax.cuh` file declares a function for computing the argmax operation on a tensor using CUDA within the `llama.cpp` codebase.
- **[argsort.cu](ggml-cuda/argsort.cu.driver.md)**: The `argsort.cu` file in the `llama.cpp` codebase implements a CUDA-based bitonic sort algorithm for sorting floating-point arrays and storing the sorted indices in an integer array, supporting both ascending and descending order.
- **[argsort.cuh](ggml-cuda/argsort.cu.driver.mdh)**: The `argsort.cuh` file declares a function for performing an argsort operation on a tensor using CUDA within the `llama.cpp` codebase.
- **[binbcast.cu](ggml-cuda/binbcast.cu.driver.md)**: The `binbcast.cu` file in the `llama.cpp` codebase implements CUDA kernels and functions for performing binary operations with broadcasting on tensors, including addition, subtraction, multiplication, division, and repeat operations.
- **[binbcast.cuh](ggml-cuda/binbcast.cu.driver.mdh)**: The `binbcast.cuh` file in the `llama.cpp` codebase declares CUDA operations for repeating, adding, subtracting, multiplying, and dividing tensors, as well as a backward operation for repeating.
- **[clamp.cu](ggml-cuda/clamp.cu.driver.md)**: The `clamp.cu` file implements a CUDA-based clamping operation for tensors, ensuring their values are constrained within specified minimum and maximum bounds.
- **[clamp.cuh](ggml-cuda/clamp.cu.driver.mdh)**: The `clamp.cuh` file defines a CUDA operation for clamping tensor values within the `llama.cpp` codebase.
- **[CMakeLists.txt](ggml-cuda/CMakeLists.txt.driver.md)**: The `CMakeLists.txt` file in the `llama.cpp/ggml/src/ggml-cuda` directory configures the build process for the CUDA components of the `ggml-cuda` library, including setting CUDA architectures, handling various compilation options, and linking necessary CUDA libraries.
- **[common.cuh](ggml-cuda/common.cuh.driver.md)**: The `common.cuh` file in the `llama.cpp` codebase provides CUDA and HIP compatibility definitions, error handling, and utility functions for GPU operations, including support for various compute capabilities and device-specific optimizations.
- **[concat.cu](ggml-cuda/concat.cu.driver.md)**: The `concat.cu` file in the `llama.cpp` codebase implements CUDA kernels for concatenating two float32 tensors along specified dimensions, handling both contiguous and non-contiguous memory layouts.
- **[concat.cuh](ggml-cuda/concat.cu.driver.mdh)**: The `concat.cuh` file in the `llama.cpp` codebase defines a CUDA operation for concatenating tensors using a specified block size.
- **[conv-transpose-1d.cu](ggml-cuda/conv-transpose-1d.cu.driver.md)**: The `conv-transpose-1d.cu` file in the `llama.cpp` codebase implements a CUDA kernel and associated functions for performing a 1D transposed convolution operation on floating-point data.
- **[conv-transpose-1d.cuh](ggml-cuda/conv-transpose-1d.cu.driver.mdh)**: The `conv-transpose-1d.cuh` file defines a CUDA operation for performing a 1D transposed convolution on a tensor within the `llama.cpp` codebase.
- **[convert.cu](ggml-cuda/convert.cu.driver.md)**: The `convert.cu` file in the `llama.cpp` codebase provides CUDA kernel implementations for dequantizing various data types and converting them to different floating-point formats, including support for different quantization schemes and data types like `Q4`, `Q5`, `Q8`, and `IQ` series.
- **[convert.cuh](ggml-cuda/convert.cu.driver.mdh)**: The `convert.cuh` file in the `llama.cpp` codebase defines CUDA function pointer types and functions for converting data to different floating-point formats, including fp32, fp16, and bf16, with support for both contiguous and non-contiguous inputs.
- **[count-equal.cu](ggml-cuda/count-equal.cu.driver.md)**: The `count-equal.cu` file in the `llama.cpp` codebase implements a CUDA kernel to count the number of equal elements between two tensors of the same type and shape.
- **[count-equal.cuh](ggml-cuda/count-equal.cu.driver.mdh)**: The `count-equal.cuh` file in the `llama.cpp` codebase defines a function for counting equal elements in a CUDA context, with a specified chunk size.
- **[cp-async.cuh](ggml-cuda/cp-async.cuh.driver.md)**: The `cp-async.cuh` file provides a simplified API for asynchronous data loading in CUDA, including functions for copying data from global to shared memory and waiting for asynchronous operations to complete.
- **[cpy.cu](ggml-cuda/cpy.cu.driver.md)**: The `cpy.cu` file in the `llama.cpp` codebase provides CUDA-based functions for copying and converting data between different tensor types, including support for various quantization formats and handling of GPU memory operations.
- **[cpy.cuh](ggml-cuda/cpy.cu.driver.mdh)**: The `cpy.cuh` file in the `llama.cpp` codebase defines functions for copying and duplicating tensors in a CUDA context, including handling destination pointers and optional indirection.
- **[cross-entropy-loss.cu](ggml-cuda/cross-entropy-loss.cu.driver.md)**: The `cross-entropy-loss.cu` file in the `llama.cpp` codebase implements CUDA kernels for computing the cross-entropy loss and its gradient, optimized for shared memory usage.
- **[cross-entropy-loss.cuh](ggml-cuda/cross-entropy-loss.cu.driver.mdh)**: The `cross-entropy-loss.cuh` file in the `llama.cpp` codebase defines functions for computing cross-entropy loss and its backward pass using CUDA.
- **[dequantize.cuh](ggml-cuda/dequantize.cuh.driver.md)**: The `dequantize.cuh` file in the `llama.cpp` codebase provides CUDA device functions for dequantizing data from various quantization formats (q4_0, q4_1, q5_0, q5_1, q8_0) using different scaling and offset techniques.
- **[diagmask.cu](ggml-cuda/diagmask.cu.driver.md)**: The `diagmask.cu` file in the `llama.cpp` codebase implements a CUDA kernel and associated functions to apply a diagonal mask with negative infinity to a float matrix, based on specified parameters.
- **[diagmask.cuh](ggml-cuda/diagmask.cu.driver.mdh)**: The `diagmask.cuh` file in the `llama.cpp` codebase defines a CUDA operation for applying a diagonal mask with infinite values using a specified block size.
- **[fattn-common.cuh](ggml-cuda/fattn-common.cuh.driver.md)**: The `fattn-common.cuh` file in the `llama.cpp` codebase provides CUDA device functions and templates for performing various vector dot product operations and dequantization tasks, specifically tailored for different quantization types and configurations, as part of a flash attention mechanism.
- **[fattn-mma-f16.cuh](ggml-cuda/fattn-mma-f16.cuh.driver.md)**: The `fattn-mma-f16.cuh` file in the `llama.cpp` codebase provides CUDA-based implementations for flash attention mechanisms using mixed-precision matrix multiplication with various configurations for different head sizes, optimizing for speed, register pressure, and shared memory usage.
- **[fattn-tile-f16.cu](ggml-cuda/fattn-tile-f16.cu.driver.md)**: The `fattn-tile-f16.cu` file in the `llama.cpp` codebase implements a CUDA kernel for performing flash attention operations on half-precision floating-point (FP16) data, specifically optimized for head sizes of 64 and 128.
- **[fattn-tile-f16.cuh](ggml-cuda/fattn-tile-f16.cu.driver.mdh)**: The `fattn-tile-f16.cuh` file declares a function for performing flash attention operations on CUDA with half-precision floating-point tiles in the `llama.cpp` codebase.
- **[fattn-tile-f32.cu](ggml-cuda/fattn-tile-f32.cu.driver.md)**: The `fattn-tile-f32.cu` file in the `llama.cpp` codebase implements a CUDA kernel for performing flash attention operations on floating-point 32 matrices, with support for specific head sizes and optional logit softcap usage.
- **[fattn-tile-f32.cuh](ggml-cuda/fattn-tile-f32.cu.driver.mdh)**: The `fattn-tile-f32.cuh` file declares a function for performing flash attention operations on CUDA with 32-bit floating point tiles in the `llama.cpp` codebase.
- **[fattn-vec-f16.cuh](ggml-cuda/fattn-vec-f16.cuh.driver.md)**: The `fattn-vec-f16.cuh` file in the `llama.cpp` codebase defines CUDA kernels and functions for performing flash attention operations on half-precision (f16) vectors, with support for various data types and configurations.
- **[fattn-vec-f32.cuh](ggml-cuda/fattn-vec-f32.cuh.driver.md)**: The `fattn-vec-f32.cuh` file in the `llama.cpp` codebase defines CUDA kernels and functions for performing flash attention operations on vectors with various data types and configurations.
- **[fattn-wmma-f16.cu](ggml-cuda/fattn-wmma-f16.cu.driver.md)**: The `fattn-wmma-f16.cu` file contains an old and deprecated implementation of WMMA FlashAttention for NVIDIA's Volta architecture, which is intended to be replaced with a dedicated Volta implementation due to changes in memory layout with Turing.
- **[fattn-wmma-f16.cuh](ggml-cuda/fattn-wmma-f16.cu.driver.mdh)**: The `fattn-wmma-f16.cuh` file declares a function for performing flash attention using WMMA with half-precision floating-point (f16) in a CUDA context.
- **[fattn.cu](ggml-cuda/fattn.cu.driver.md)**: The `fattn.cu` file in the `llama.cpp` codebase implements CUDA-based flash attention mechanisms with various optimizations and configurations for different hardware capabilities and data types.
- **[fattn.cuh](ggml-cuda/fattn.cu.driver.mdh)**: The `fattn.cuh` file in the `llama.cpp` codebase declares a function for performing flash attention using CUDA.
- **[getrows.cu](ggml-cuda/getrows.cu.driver.md)**: The `getrows.cu` file in the `llama.cpp` codebase implements CUDA kernels and functions for retrieving and processing rows of data, including dequantization and handling different data types, within a GPU context.
- **[getrows.cuh](ggml-cuda/getrows.cu.driver.mdh)**: The `getrows.cuh` file in the `llama.cpp` codebase defines CUDA functions for retrieving rows from tensors and includes operations for both forward and backward row retrieval.
- **[ggml-cuda.cu](ggml-cuda/ggml-cuda.cu.driver.md)**: The `ggml-cuda.cu` file in the `llama.cpp` codebase provides CUDA implementations for various tensor operations, including matrix multiplication, element-wise operations, and memory management, specifically optimized for GPU execution.
- **[gla.cu](ggml-cuda/gla.cu.driver.md)**: The `gla.cu` file in the `llama.cpp` codebase implements a CUDA kernel for performing gated linear attention operations on tensors.
- **[gla.cuh](ggml-cuda/gla.cu.driver.mdh)**: The `gla.cuh` file declares a function for performing gated linear attention operations using CUDA within the `llama.cpp` codebase.
- **[im2col.cu](ggml-cuda/im2col.cu.driver.md)**: The `im2col.cu` file in the `llama.cpp` codebase implements CUDA kernels and functions for the im2col operation, which is used to transform image data into column format for convolution operations.
- **[im2col.cuh](ggml-cuda/im2col.cu.driver.mdh)**: The `im2col.cuh` file in the `llama.cpp` codebase defines a CUDA operation for the im2col transformation with a specified block size.
- **[mma.cuh](ggml-cuda/mma.cuh.driver.md)**: The `mma.cuh` file in the `llama.cpp` codebase provides CUDA primitives for matrix multiplication using tensor core PTX instructions, with a focus on defining memory layouts and operations for different matrix tile configurations.
- **[mmq.cu](ggml-cuda/mmq.cu.driver.md)**: The `mmq.cu` file in the `llama.cpp` codebase implements CUDA-based matrix multiplication for various quantized data types, providing functions to handle different quantization cases and determining when to use matrix multiplication with quantization (MMQ) based on the GPU architecture.
- **[mmq.cuh](ggml-cuda/mmq.cu.driver.mdh)**: The `mmq.cuh` file in the `llama.cpp` codebase provides CUDA-based matrix multiplication kernels optimized for various quantized data types, supporting both regular and MoE (Mixture of Experts) matrix multiplications with stream-k work partitioning.
- **[mmv.cu](ggml-cuda/mmv.cu.driver.md)**: The `mmv.cu` file in the `llama.cpp` codebase implements CUDA-based matrix-vector multiplication operations, supporting various data types and optimizing for different block sizes and device capabilities.
- **[mmv.cuh](ggml-cuda/mmv.cu.driver.mdh)**: The `mmv.cuh` file in the `llama.cpp` codebase defines functions for performing matrix-vector multiplication using CUDA, with specific optimizations for FP16 tensor cores.
- **[mmvq.cu](ggml-cuda/mmvq.cu.driver.md)**: The `mmvq.cu` file in the `llama.cpp` codebase implements CUDA-based matrix-vector multiplication with quantization support, providing functions to handle different quantization types and device-specific optimizations.
- **[mmvq.cuh](ggml-cuda/mmvq.cu.driver.mdh)**: The `mmvq.cuh` file in the `llama.cpp` codebase defines CUDA functions for performing matrix-vector multiplication operations with specific kernel configurations.
- **[norm.cu](ggml-cuda/norm.cu.driver.md)**: The `norm.cu` file in the `llama.cpp` codebase implements various CUDA-based normalization operations, including standard, group, RMS, and L2 normalization, for processing tensors on the GPU.
- **[norm.cuh](ggml-cuda/norm.cu.driver.mdh)**: The `norm.cuh` file in the `llama.cpp` codebase defines CUDA operations for various normalization techniques, including group norm, RMS norm, and L2 norm.
- **[opt-step-adamw.cu](ggml-cuda/opt-step-adamw.cu.driver.md)**: The `opt-step-adamw.cu` file in the `llama.cpp` codebase implements a CUDA kernel for performing the AdamW optimization step on floating-point tensors.
- **[opt-step-adamw.cuh](ggml-cuda/opt-step-adamw.cu.driver.mdh)**: The `opt-step-adamw.cuh` file in the `llama.cpp` codebase defines a CUDA function for performing an optimization step using the AdamW algorithm.
- **[out-prod.cu](ggml-cuda/out-prod.cu.driver.md)**: The `out-prod.cu` file in the `llama.cpp` codebase implements a CUDA-based outer product operation for tensors using cuBLAS, with support for specific data type and dimensionality assertions.
- **[out-prod.cuh](ggml-cuda/out-prod.cu.driver.mdh)**: The `out-prod.cuh` file declares a function for performing an outer product operation using CUDA within the `llama.cpp` codebase.
- **[pad.cu](ggml-cuda/pad.cu.driver.md)**: The `pad.cu` file in the `llama.cpp` codebase implements a CUDA kernel for padding 3D tensors with zeros, ensuring compatibility with the specified dimensions.
- **[pad.cuh](ggml-cuda/pad.cu.driver.mdh)**: The `pad.cuh` file in the `llama.cpp` codebase defines a CUDA operation for padding tensors with a specified block size.
- **[pool2d.cu](ggml-cuda/pool2d.cu.driver.md)**: The `pool2d.cu` file in the `llama.cpp` codebase implements CUDA kernels for performing 2D pooling operations, such as average and max pooling, on tensors in NCHW format.
- **[pool2d.cuh](ggml-cuda/pool2d.cu.driver.mdh)**: The `pool2d.cuh` file in the `llama.cpp` codebase defines a CUDA operation for 2D pooling with a specified block size.
- **[quantize.cu](ggml-cuda/quantize.cu.driver.md)**: The `quantize.cu` file in the `llama.cpp` codebase implements CUDA kernels for quantizing floating-point data into 8-bit integer representations, supporting different data layout strategies.
- **[quantize.cuh](ggml-cuda/quantize.cu.driver.mdh)**: The `quantize.cuh` file in the `llama.cpp` codebase defines CUDA functions and constants for quantizing data, including specific implementations for row and matrix-matrix quantization.
- **[rope.cu](ggml-cuda/rope.cu.driver.md)**: The `rope.cu` file in the `llama.cpp` codebase implements CUDA-based operations for rotary positional embeddings, including various algorithms like YaRN, for different modes such as Neox, multi-dimensional, and vision, using templates and device functions.
- **[rope.cuh](ggml-cuda/rope.cu.driver.mdh)**: The `rope.cuh` file in the `llama.cpp` codebase defines CUDA operations for the `ggml_cuda_op_rope` and `ggml_cuda_op_rope_back` functions, which likely perform operations related to the "rope" concept on GPU tensors.
- **[scale.cu](ggml-cuda/scale.cu.driver.md)**: The `scale.cu` file in the `llama.cpp` codebase implements a CUDA-based function to scale elements of a float array by a given factor.
- **[scale.cuh](ggml-cuda/scale.cu.driver.mdh)**: The `scale.cuh` file in the `llama.cpp` codebase defines a CUDA operation for scaling tensors using a specified block size.
- **[softmax.cu](ggml-cuda/softmax.cu.driver.md)**: The `softmax.cu` file in the `llama.cpp` codebase implements CUDA-based forward and backward operations for the softmax function, including support for optional masking and scaling.
- **[softmax.cuh](ggml-cuda/softmax.cu.driver.mdh)**: The `softmax.cuh` file in the `llama.cpp` codebase defines CUDA operations for computing the softmax function and its backward pass on tensors.
- **[ssm-conv.cu](ggml-cuda/ssm-conv.cu.driver.md)**: The `ssm-conv.cu` file in the `llama.cpp` codebase implements CUDA kernels for performing single-step convolution operations on floating-point data, specifically designed for sequences with a kernel size of 4.
- **[ssm-conv.cuh](ggml-cuda/ssm-conv.cu.driver.mdh)**: The `ssm-conv.cuh` file declares a CUDA operation function `ggml_cuda_op_ssm_conv` for performing convolution on a tensor within the `llama.cpp` codebase.
- **[ssm-scan.cu](ggml-cuda/ssm-scan.cu.driver.md)**: The `ssm-scan.cu` file in the `llama.cpp` codebase implements a CUDA kernel for performing a scan operation on multiple input tensors, specifically designed for processing sequences with a fixed size of 16, and includes a function to launch this kernel on a CUDA stream.
- **[ssm-scan.cuh](ggml-cuda/ssm-scan.cu.driver.mdh)**: The `ssm-scan.cuh` file declares a function for performing a CUDA operation related to SSM scanning within the `llama.cpp` codebase.
- **[sum.cu](ggml-cuda/sum.cu.driver.md)**: The `sum.cu` file in the `llama.cpp` codebase implements CUDA-based summation operations for floating-point tensors, utilizing the CUB library for efficient reduction when available.
- **[sum.cuh](ggml-cuda/sum.cu.driver.mdh)**: The `sum.cuh` file in the `llama.cpp` codebase declares functions for performing summation operations on CUDA, including `sum_f32_cuda` and `ggml_cuda_op_sum`.
- **[sumrows.cu](ggml-cuda/sumrows.cu.driver.md)**: The `sumrows.cu` file implements a CUDA kernel and associated functions to sum the rows of a matrix of 32-bit floats in the `llama.cpp` codebase.
- **[sumrows.cuh](ggml-cuda/sumrows.cu.driver.mdh)**: The `sumrows.cuh` file declares functions for summing rows of a matrix using CUDA in the `llama.cpp` codebase.
- **[tsembd.cu](ggml-cuda/tsembd.cu.driver.md)**: The `tsembd.cu` file in the `llama.cpp` codebase implements a CUDA kernel for computing timestep embeddings using sinusoidal functions.
- **[tsembd.cuh](ggml-cuda/tsembd.cu.driver.mdh)**: The `tsembd.cuh` file in the `llama.cpp` codebase defines a CUDA operation for timestep embedding with a specified block size.
- **[unary.cu](ggml-cuda/unary.cu.driver.md)**: The `unary.cu` file in the `llama.cpp` codebase implements various CUDA-based unary operations for tensors, including mathematical functions like absolute value, sign, negation, step, GELU, SiLU, tanh, ReLU, sigmoid, and others, as well as their corresponding CUDA kernel functions.
- **[unary.cuh](ggml-cuda/unary.cu.driver.mdh)**: The `unary.cuh` file in the `llama.cpp` codebase defines CUDA block sizes and declares functions for various unary operations on tensors, such as absolute value, sign, negation, and several activation functions.
- **[upscale.cu](ggml-cuda/upscale.cu.driver.md)**: The `upscale.cu` file in the `llama.cpp` codebase implements a CUDA-based function to upscale a 4-dimensional float tensor by specified scaling factors.
- **[upscale.cuh](ggml-cuda/upscale.cu.driver.mdh)**: The `upscale.cuh` file in the `llama.cpp` codebase defines a CUDA operation for upscaling tensors with a specified block size.
- **[vecdotq.cuh](ggml-cuda/vecdotq.cuh.driver.md)**: The `vecdotq.cuh` file in the `llama.cpp` codebase provides CUDA device functions for performing vector dot product operations on quantized data, utilizing various quantization schemes and SIMD operations to optimize performance.
- **[wkv.cu](ggml-cuda/wkv.cu.driver.md)**: The `wkv.cu` file in the `llama.cpp` codebase implements CUDA kernels for the RWKV model, specifically for the `rwkv_wkv_f32` and `rwkv_wkv7_f32` functions, which perform operations on tensors using GPU parallelization.
- **[wkv.cuh](ggml-cuda/wkv.cu.driver.mdh)**: The `wkv.cuh` file in the `llama.cpp` codebase defines CUDA operations for RWKV models with functions `ggml_cuda_op_rwkv_wkv6` and `ggml_cuda_op_rwkv_wkv7`.
