## Folders
- **[nix](.devops/nix.driver.md)**: The `nix` folder in the `llama.cpp` codebase contains various Nix expressions and configurations for building and managing applications, development environments, Docker and Singularity images, and platform-specific settings, including support for NVIDIA Jetson platforms and Python package management.

## Files
- **[cloud-v-pipeline](.devops/cloud-v-pipeline.driver.md)**: The `cloud-v-pipeline` file defines a CI/CD pipeline for compiling and running the `llama.cpp` project on an x86 runner with RISC-V support using vector qemu and gcc.
- **[cpu.Dockerfile](.devops/cpu.Dockerfile.driver.md)**: The `cpu.Dockerfile` file in the `llama.cpp` codebase defines a multi-stage Docker build process for creating different deployment images (full, light, and server) based on Ubuntu, tailored for various CPU architectures.
- **[cuda.Dockerfile](.devops/cuda.Dockerfile.driver.md)**: The `cuda.Dockerfile` file in the `llama.cpp` codebase defines a multi-stage Docker build process for creating CUDA-enabled development, runtime, full, light, and server images based on Ubuntu 22.04 and CUDA 12.4.0.
- **[intel.Dockerfile](.devops/intel.Dockerfile.driver.md)**: The `intel.Dockerfile` file in the `llama.cpp` codebase defines a multi-stage Docker build process for creating different deployment images (full, light, and server) using Intel's oneAPI basekit and various build configurations.
- **[llama-cli-cann.Dockerfile](.devops/llama-cli-cann.Dockerfile.driver.md)**: The `llama-cli-cann.Dockerfile` sets up a Docker environment for building and running the `llama-cli` application using the Ascend AI CANN toolkit.
- **[llama-cpp-cuda.srpm.spec](.devops/llama-cpp-cuda.srpm.spec.driver.md)**: The `llama-cpp-cuda.srpm.spec` file is a specification for building and packaging an RPM for the `llama.cpp` project, focusing on CPU inference of LLaMA models with CUDA support, and includes details on dependencies, build instructions, and service configuration.
- **[llama-cpp.srpm.spec](.devops/llama-cpp.srpm.spec.driver.md)**: The `llama-cpp.srpm.spec` file in the `llama.cpp` codebase provides the specifications for building and packaging the LLaMA model CPU inference software as an RPM for RPM-based distributions, including details on dependencies and installation instructions.
- **[musa.Dockerfile](.devops/musa.Dockerfile.driver.md)**: The `musa.Dockerfile` file in the `llama.cpp` codebase defines a multi-stage Docker build process for creating development, full, light, and server images using the MUSA framework on Ubuntu 22.04.
- **[rocm.Dockerfile](.devops/rocm.Dockerfile.driver.md)**: The `rocm.Dockerfile` in the `llama.cpp` codebase defines a multi-stage Docker build process for creating various ROCm-based images, including full, light, and server configurations, targeting different AMD GPU architectures.
- **[tools.sh](.devops/tools.sh.driver.md)**: The `tools.sh` file in the `llama.cpp` codebase provides a command-line interface for various operations such as converting, quantizing, running, benchmarking, and measuring perplexity of llama models, as well as running a model on a server.
- **[vulkan.Dockerfile](.devops/vulkan.Dockerfile.driver.md)**: The `vulkan.Dockerfile` file in the `llama.cpp` codebase defines a multi-stage Docker build process for setting up an Ubuntu-based environment with Vulkan SDK and other dependencies, and provides configurations for full, light, and server versions of the application.
