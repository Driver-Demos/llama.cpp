# Purpose
The provided content outlines the continuous integration (CI) setup for the `llama.cpp` project, which utilizes both GitHub Actions and a custom CI framework hosted at `https://github.com/ggml-org/ci`. This custom framework monitors the `master` branch for new commits and executes the `ci/run.sh` script on dedicated cloud instances, allowing for more resource-intensive workloads and scalability across different hardware architectures, such as GPU and Apple Silicon. Collaborators can trigger the CI process by including the `ggml-ci` keyword in their commit messages, but this is limited to branches within the repository. The document also provides instructions for running the CI locally, including configurations for CPU-only, CUDA, SYCL, and MUSA support, and details the process for executing the MUSA CI within a Docker container, ensuring an isolated environment while preserving cached data and results.
