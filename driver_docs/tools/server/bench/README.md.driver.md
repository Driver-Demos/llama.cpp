# Purpose
This document is a comprehensive guide for setting up and running server benchmarks using the k6 tool, specifically tailored for evaluating chat completion models. It provides detailed instructions on installing necessary tools and extensions, downloading datasets and models, and configuring the server to handle OpenAI Chat completion requests. The file outlines the process for executing benchmarks, including setting parameters such as the number of concurrent users, duration, and iterations, and offers options to customize these benchmarks through environment variables. Additionally, it describes the metrics available for analysis, which are derived from the chat completions' responses, and provides guidance on using a Python script for continuous integration (CI) to automate the benchmarking process. This file is crucial for developers and testers in a codebase focused on performance evaluation and optimization of AI models, ensuring consistent and reproducible benchmarking results.
# Content Summary
This document provides a comprehensive guide for setting up and running server benchmarks using the k6 tool, specifically tailored for evaluating server performance in handling OpenAI Chat completion requests. The document outlines the necessary steps to install k6 along with the xk6-sse extension, which is required for handling Server-Sent Events (SSE) not natively supported by k6. It provides a command to build k6 with this extension, assuming Go language version 1.21 or higher is installed.

The document also details the process of downloading a dataset and a model, which are essential for running the benchmarks. The dataset is sourced from the vLLM benchmarks, and the model is retrieved using a script that fetches it from a specified repository.

To start the server, the document provides a command that configures the server to respond to chat completion requests at a specified URL, either the default `http://localhost:8080/v1` or a custom URL set via the `SERVER_BENCH_URL` environment variable. The server is configured with various parameters such as model file, parallel processing, batch size, and context size.

The benchmark execution is described using k6, where a sample command is provided to simulate 500 chat completion requests with 8 concurrent users over a maximum duration of 10 minutes. The document lists several environment variables that can override default benchmark values, such as the server URL, number of prompts, model alias, maximum tokens, dataset path, and context size.

Metrics are a crucial part of the benchmarking process, and the document specifies several metrics computed from the OAI chat completions response, including trends and counters for tokens and rates of completions that are truncated or stopped. It also mentions the use of k6 options for debugging HTTP requests and suggests comparing k6 metrics with server metrics using a curl command.

Finally, the document introduces a Python script, `bench.py`, designed for continuous integration (CI) environments. This script automates the process of starting the server, configuring k6 variables, running the k6 script, and extracting metrics from Prometheus. It can also be executed manually with specified parameters for server binary path, runner label, scenario, and various model and benchmark configurations.
