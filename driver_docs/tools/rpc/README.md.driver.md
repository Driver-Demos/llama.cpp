# Purpose
This document is a configuration and usage guide for setting up a Remote Procedure Call (RPC) server to facilitate distributed inference for large language models (LLMs) using the `llama.cpp` framework. The file provides detailed instructions on how to build and run the `rpc-server` on different hosts, each potentially using different computational backends like CUDA or Metal, to offload computations from the main host. The document includes a flowchart illustrating the network communication between the main host and multiple remote hosts, emphasizing the distributed nature of the setup. It also covers the configuration of local caching to optimize performance by reducing network data transfer. This file is crucial for developers looking to leverage distributed computing resources to enhance the performance of LLM inference tasks, ensuring they correctly configure and deploy the necessary components across various environments.
# Content Summary
The provided content is a documentation excerpt for setting up and using an RPC (Remote Procedure Call) server to facilitate distributed inference with the `llama.cpp` library, specifically for large language models (LLMs). The document highlights that the RPC server and backend are in a proof-of-concept stage, emphasizing the importance of not deploying the server in open or sensitive environments due to potential security vulnerabilities.

The `rpc-server` is designed to run the `ggml` backend on remote hosts, allowing computations to be offloaded to multiple instances of `rpc-server`. This setup supports distributed LLM inference by enabling different hosts to run various backends, such as CUDA or Metal, and even allows multiple `rpc-server` instances on a single host, each potentially using a different backend.

To use the RPC server, developers must build the backend on each host using `cmake`, with the `-DGGML_RPC=ON` option to enable RPC support. The document provides a step-by-step guide for building the CUDA backend and starting the `rpc-server`, including specifying the port and CUDA device. The `CUDA_VISIBLE_DEVICES` environment variable is used to select specific CUDA devices, facilitating multiple server instances on the same host.

On the main host, `llama.cpp` must be built with RPC support, and the `llama-cli` command is used to run the model, specifying the RPC servers' host and port using the `--rpc` option. This setup allows model layers to be offloaded to both local and remote devices, enhancing computational distribution.

Additionally, the document describes a local caching mechanism for the RPC server, which can store large tensors to reduce network transfer times and speed up model loading. The cache is enabled with the `-c` option and is stored by default in the `$HOME/.cache/llama.cpp/rpc` directory, with the location configurable via the `LLAMA_CACHE` environment variable. This feature is particularly beneficial when working with large models, as it optimizes performance by minimizing data transfer overhead.
