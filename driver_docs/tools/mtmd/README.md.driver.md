# Purpose
The provided content is a documentation file detailing the multimodal support capabilities within the `llama.cpp` project, which is a software codebase designed to handle various vision-capable models. This file serves a broad functionality by explaining the evolution and current state of multimodal support, highlighting the transition from the initial LLaVA model to a more comprehensive system that includes multiple vision models. It outlines the development history, the introduction of `libmtmd` as a unified library for handling multimodal inputs, and the creation of `mmproj` files necessary for encoding images into embeddings for language models. The documentation is crucial for developers and users of the `llama.cpp` project, as it provides guidance on the configuration and usage of multimodal models, ensuring they understand the necessary components and tools required for effective implementation.
# Content Summary
The document provides an overview of the multimodal support capabilities within the `llama.cpp` project, which initially focused on LLaVA models but has since expanded to include a variety of vision-capable models. This expansion has led to the development of a sub-project within `llama.cpp` that is under active development, with potential for breaking changes. The document outlines the evolution of multimodal support, highlighting key milestones such as the introduction of support for MobileVLM and the subsequent addition of various other models. This growth resulted in a fragmented command-line interface (CLI) landscape, prompting the creation of `libmtmd` and `mtmd-cli` to unify and simplify user interaction across different models.

The document explains the technical approach to multimodal support, which involves encoding images into embeddings using a separate model component before feeding them into the language model. This separation allows for independent development cycles and accommodates the diverse preprocessing and projection steps of modern vision models. To run a multimodal model, two GGUF files are typically required: a standard language model file and a multimodal projector (`mmproj`) file.

`libmtmd` is introduced as a modern library designed to replace the original `llava.cpp` implementation, offering a unified interface, improved user and developer experience, and support for multiple input types. The document also provides guidance on obtaining `mmproj` files for various models, detailing the use of the `convert_hf_to_gguf.py` script with the `--mmproj` flag for specific architectures. Additionally, it references conversion scripts for older models, located under `tools/mtmd/legacy-models`, and provides links to relevant guides for further instructions.
