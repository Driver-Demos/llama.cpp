## Folders
- **[batched](examples/batched.driver.md)**: The `batched` folder in the `llama.cpp` codebase contains files for demonstrating and configuring batched text generation using a language model, including source code, build configuration, and usage instructions.
- **[batched.swift](examples/batched.driver.md.swift)**: The `batched.swift` folder in the `llama.cpp` codebase contains a Swift-based implementation for parallel text generation using the Llama library, with supporting files for build automation, package configuration, and usage instructions.
- **[convert-llama2c-to-ggml](examples/convert-llama2c-to-ggml.driver.md)**: The `convert-llama2c-to-ggml` folder in the `llama.cpp` codebase contains files for building and executing a tool that converts Llama2.c models to the GGML format, including configuration, conversion logic, and usage instructions.
- **[deprecation-warning](examples/deprecation-warning.driver.md)**: The `deprecation-warning` folder in the `llama.cpp` codebase contains files that inform users about the deprecation and renaming of binary filenames, offering guidance on updating scripts and workflows.
- **[embedding](examples/embedding.driver.md)**: The `embedding` folder in the `llama.cpp` codebase contains files for configuring, implementing, and documenting the process of generating embeddings from text using the LLaMA model.
- **[eval-callback](examples/eval-callback.driver.md)**: The `eval-callback` folder in the `llama.cpp` codebase contains files for configuring, implementing, and demonstrating a callback mechanism for evaluating tensor operations during graph execution.
- **[gen-docs](examples/gen-docs.driver.md)**: The `gen-docs` folder in the `llama.cpp` codebase contains files for configuring and generating markdown documentation for command-line arguments, including a CMake configuration file and a C++ source file.
- **[gguf](examples/gguf.driver.md)**: The `gguf` folder in the `llama.cpp` codebase contains configuration and source files for building and implementing functionality related to GGUF file handling, including a CMake configuration and a C++ source file for reading and writing these files.
- **[gguf-hash](examples/gguf.driver.md-hash)**: The `gguf-hash` folder in the `llama.cpp` codebase contains the implementation and build configuration for a command-line tool that generates and verifies hashes for GGUF files using various algorithms, with dependencies managed in the `deps` folder.
- **[gritlm](examples/gritlm.driver.md)**: The `gritlm` folder in the `llama.cpp` codebase contains files for configuring, implementing, and providing instructions for using the GritLM model, including a CMake configuration file, an example implementation of the model, and a README with usage instructions.
- **[jeopardy](examples/jeopardy.driver.md)**: The `jeopardy` folder in the `llama.cpp` codebase contains scripts and data files for running and analyzing a Jeopardy game to evaluate and compare the performance of different language models.
- **[llama.android](examples/llama.android.driver.md)**: The `llama.android` folder in the `llama.cpp` codebase is dedicated to developing an Android application that integrates the Llama model, featuring essential components like build configurations, source files, and Gradle settings.
- **[llama.swiftui](examples/llama.swiftui.driver.md)**: The `llama.swiftui` folder in the `llama.cpp` codebase contains resources and configurations for a SwiftUI application that manages machine learning models, including Swift implementations, UI components, Xcode project settings, and documentation for building and running the app on an iPhone.
- **[lookahead](examples/lookahead.driver.md)**: The `lookahead` folder in the `llama.cpp` codebase contains files for configuring, implementing, and documenting a lookahead decoding example using n-grams for token prediction in a language model.
- **[lookup](examples/lookup.driver.md)**: The `lookup` folder in the `llama.cpp` codebase contains source files and configuration for building and demonstrating various utilities related to Prompt Lookup Decoding, including tokenization, n-gram caching, and statistics gathering, as well as a README for guidance.
- **[parallel](examples/parallel.driver.md)**: The `parallel` folder in the `llama.cpp` codebase contains files for configuring, implementing, and documenting a parallel processing simulation of client requests using the `llama-parallel` tool.
- **[passkey](examples/passkey.driver.md)**: The `passkey` folder in the `llama.cpp` codebase contains files for configuring, implementing, and documenting an example program that evaluates a language model's ability to recall a passkey from long text contexts.
- **[retrieval](examples/retrieval.driver.md)**: The `retrieval` folder in the `llama.cpp` codebase contains files for building and using a retrieval system based on cosine similarity, including configuration, documentation, and implementation components.
- **[save-load-state](examples/save-load-state.driver.md)**: The `save-load-state` folder in the `llama.cpp` codebase contains files for configuring and demonstrating the saving and loading of a language model's state to ensure consistent text generation.
- **[simple](examples/simple.driver.md)**: The `simple` folder in the `llama.cpp` codebase contains files that configure the build process, provide usage instructions, and demonstrate a minimal example of using the LLaMA model for text generation.
- **[simple-chat](examples/simple.driver.md-chat)**: The `simple-chat` folder in the `llama.cpp` codebase contains files necessary for building and running a command-line chat application using the LLaMA framework, including build configuration, usage instructions, and the main implementation.
- **[simple-cmake-pkg](examples/simple.driver.md-cmake-pkg)**: The `simple-cmake-pkg` folder in the `llama.cpp` codebase contains configuration and documentation files for building and running a simple example program using a relocatable CMake package with the Llama library.
- **[speculative](examples/speculative.driver.md)**: The `speculative` folder in the `llama.cpp` codebase contains files for building and demonstrating speculative decoding techniques, including a CMake configuration, a README with explanations and links, and an implementation of speculative sampling in `speculative.cpp`.
- **[speculative-simple](examples/speculative.driver.md-simple)**: The `speculative-simple` folder in the `llama.cpp` codebase contains files for building and demonstrating a basic greedy speculative decoding example using the llama model, including configuration, instructions, and implementation.
- **[sycl](examples/sycl.driver.md)**: The `sycl` folder in the `llama.cpp` codebase contains scripts, configuration files, and a program for building and running SYCL-based examples and models using Intel's oneAPI, with support for both Linux and Windows environments.
- **[training](examples/training.driver.md)**: The `training` folder in the `llama.cpp` codebase contains files for configuring, implementing, and documenting the process of fine-tuning language models, including a build configuration file, a source code file for the fine-tuning process, and a README with setup instructions.

## Files
- **[chat-13B.bat](examples/chat-13B.bat.driver.md)**: The `chat-13B.bat` file is a batch script for setting up and running a chat interaction with an AI model in the `llama.cpp` codebase, configuring parameters such as model path, user and AI names, and generation options.
- **[chat-13B.sh](examples/chat-13B.sh.driver.md)**: The `chat-13B.sh` file is a bash script for setting up and running a chat interaction with the ChatLLaMa model using specified parameters and options in the `llama.cpp` codebase.
- **[chat-persistent.sh](examples/chat-persistent.sh.driver.md)**: The `chat-persistent.sh` file is a Bash script in the `llama.cpp` codebase that manages a persistent chat session with a language model, handling prompt caching and context management.
- **[chat-vicuna.sh](examples/chat-vicuna.sh.driver.md)**: The `chat-vicuna.sh` file is a bash script for setting up and running an interactive chat session with a Vicuna model using the `llama.cpp` framework, allowing customization of model parameters and prompt templates.
- **[chat.sh](examples/chat.sh.driver.md)**: The `chat.sh` file is a temporary script for running a chat example using the `llama-cli` with specific model and prompt configurations in the `llama.cpp` codebase.
- **[CMakeLists.txt](examples/CMakeLists.txt.driver.md)**: The `CMakeLists.txt` file in the `llama.cpp/examples` directory configures the build process for various example subdirectories, handling dependencies, compile flags, and conditional inclusion based on the build environment.
- **[convert_legacy_llama.py](examples/convert_legacy_llama.py.driver.md)**: The `convert_legacy_llama.py` file in the `llama.cpp` codebase provides a comprehensive script for converting LLaMA models to a GGML-compatible format, including handling various data types, loading models, managing vocabularies, and writing output files with metadata.
- **[json_schema_pydantic_example.py](examples/json_schema_pydantic_example.py.driver.md)**: The `json_schema_pydantic_example.py` file demonstrates how to create chat completions using an OpenAI-compatible endpoint with JSON schema support, utilizing Pydantic models for response validation.
- **[json_schema_to_grammar.py](examples/json_schema_to_grammar.py.driver.md)**: The `json_schema_to_grammar.py` file in the `llama.cpp` codebase provides a script that converts a JSON schema into a grammar suitable for use with the `llama-cli`, supporting a subset of JSON schema features and allowing for optional fetching of remote schemas.
- **[llama.vim](examples/llama.vim.driver.md)**: The `llama.vim` file provides a Vim/Neovim plugin for LLM-based text completion using a llama.cpp server, requiring configuration and setup for integration with text editors and server instances.
- **[llm.vim](examples/llm.vim.driver.md)**: The `llm.vim` file is a basic Vim plugin example that sends the current buffer content to a local server for text completion and inserts the response back into the buffer.
- **[Miku.sh](examples/Miku.sh.driver.md)**: The `Miku.sh` file is a bash script that sets up and runs an interactive conversation between a user and an AI assistant named Miku using the llama-cli tool with specified model and generation options.
- **[pydantic_models_to_grammar.py](examples/pydantic_models_to_grammar.py.driver.md)**: The `pydantic_models_to_grammar.py` file in the `llama.cpp` codebase provides functionality to convert Pydantic models into Generalized Backus-Naur Form (GBNF) grammar rules, generate documentation for these models, and save the generated grammar and documentation to files.
- **[pydantic_models_to_grammar_examples.py](examples/pydantic_models_to_grammar_examples.py.driver.md)**: The `pydantic_models_to_grammar_examples.py` file demonstrates the use of pydantic models to create grammar-based function calling examples, including tasks like sending messages, performing calculations, structuring book data, and concurrent function execution with a language model.
- **[reason-act.sh](examples/reason-act.sh.driver.md)**: The `reason-act.sh` file is a Bash script that runs the `llama-cli` command with specified parameters to process prompts from `reason-act.txt` in an interactive mode.
- **[regex_to_grammar.py](examples/regex_to_grammar.py.driver.md)**: The `regex_to_grammar.py` file executes a subprocess to convert a regex pattern into a grammar format using a script named `json_schema_to_grammar.py`.
- **[server-llama2-13B.sh](examples/server-llama2-13B.sh.driver.md)**: The `server-llama2-13B.sh` file is a shell script for setting up and running a server using the Llama 2 13B model with configurable options for model path, prompt template, CPU threads, and generation parameters.
- **[server_embd.py](examples/server_embd.py.driver.md)**: The `server_embd.py` file demonstrates an asynchronous client that sends multiple POST requests to a local server to obtain embeddings and then calculates the cosine similarity between these embeddings.
- **[ts-type-to-grammar.sh](examples/ts-type-to-grammar.sh.driver.md)**: The `ts-type-to-grammar.sh` file is a shell script that converts a TypeScript type definition into a JSON schema and then processes it into a grammar using a Python script.
