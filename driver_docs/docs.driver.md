## Folders
- **[backend](docs/backend.driver.md)**: The `backend` folder in the `llama.cpp` codebase contains documentation files that provide detailed installation and setup guides for various software frameworks and architectures, including BLIS, CANN, CUDA on Fedora, OpenCL, and SYCL, to support different hardware and operating systems.
- **[development](docs/development.driver.md)**: The `development` folder in the `llama.cpp` codebase contains documentation files that provide guidance on debugging tests, adding new model architectures, and optimizing token generation performance.
- **[multimodal](docs/multimodal.driver.md)**: The `multimodal` folder in the `llama.cpp` codebase contains documentation files providing detailed instructions for setting up, converting, and running various multimodal models, including Gemma 3, GLMV-EDGE, Granite Vision, LLaVA, MiniCPM, and MobileVLM, within the `llama.cpp` framework.

## Files
- **[android.md](docs/android.md.driver.md)**: The `android.md` file in the `llama.cpp` codebase provides detailed instructions for building and running `llama.cpp` on Android devices using Termux and cross-compiling with the Android NDK.
- **[build.md](docs/build.md.driver.md)**: The `build.md` file in the `llama.cpp` codebase provides detailed instructions for building the `llama` library locally, including various configurations for different backends such as CPU, BLAS, Metal, SYCL, CUDA, MUSA, HIP, Vulkan, CANN, Arm KleidiAI, and OpenCL, as well as specific instructions for building on Windows, Linux, and Android platforms.
- **[docker.md](docs/docker.md.driver.md)**: The `docker.md` file in the `llama.cpp` codebase provides detailed instructions on using and building various Docker images for the project, including configurations for CUDA, ROCm, MUSA, and Intel SYCL support, as well as usage examples for running LLaMA models.
- **[function-calling.md](docs/function-calling.md.driver.md)**: The `function-calling.md` file in the `llama.cpp` codebase provides documentation on the support for OpenAI-style function calling, detailing the models and formats supported, usage instructions, and examples of how to implement function calling with various templates and handlers.
- **[install.md](docs/install.md.driver.md)**: The `install.md` file in the `llama.cpp` codebase provides instructions for installing the pre-built version of `llama.cpp` on Windows, Mac, and Linux using various package managers like Winget, Homebrew, MacPorts, and Nix.
- **[llguidance.md](docs/llguidance.md.driver.md)**: The `llguidance.md` file in the `llama.cpp` codebase provides documentation on integrating and using the LLGuidance library for constrained decoding with Large Language Models, detailing its support for JSON Schemas, context-free grammars, and the necessary build process involving the Rust compiler.
- **[multimodal.md](docs/multimodal.driver.md.md)**: The `multimodal.md` file in the `llama.cpp` codebase provides documentation on how to enable and use multimodal input, supporting image and experimental audio inputs, via the `libmtmd` library with tools like `llama-mtmd-cli` and `llama-server`.
