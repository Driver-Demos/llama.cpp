# Purpose
The provided content is a documentation snippet from the `llama.cpp/examples/training` directory, which is part of a software codebase focused on language model training using the llama.cpp framework and GGML (General Graphical Model Language). This document outlines the current capabilities and limitations of the finetuning process, specifically for FP32 models and certain hardware configurations. It provides guidance on compiling the software for CPU and CUDA-based training, emphasizing the need to adjust settings based on the hardware used. Additionally, it includes a proof-of-concept script for finetuning a language model using the Wikitext-2 dataset, with the expectation that the model's perplexity—a measure of how well a probability model predicts a sample—should decrease after training. This documentation serves as a guide for users to experiment with and understand the finetuning process within this specific framework.
