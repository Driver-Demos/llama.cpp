
## Files
- **[batched.cpp](batched/batched.cpp.driver.md)**: The `batched.cpp` file in the `llama.cpp` codebase demonstrates how to initialize and use a language model to generate text sequences in parallel batches, including setting up the model, tokenizing input, and decoding output.
- **[CMakeLists.txt](batched/CMakeLists.txt.driver.md)**: The `CMakeLists.txt` file in `llama.cpp/examples/batched` configures the build system to compile and install the `llama-batched` executable, linking it with the `common`, `llama`, and threading libraries, and requiring C++17 standard features.
- **[README.md](batched/README.md.driver.md)**: The `README.md` file in `llama.cpp/examples/batched` provides instructions and an example of how to perform batched text generation using the `llama-batched` command with a specified model and prompt.
