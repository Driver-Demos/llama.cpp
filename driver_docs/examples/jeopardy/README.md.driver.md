# Purpose
The provided content is a set of instructions for running a Jeopardy-style test to evaluate and compare the factual knowledge of different language models using the llama.cpp framework. This process involves configuring a shell script, `jeopardy.sh`, by specifying the model's path, name, and any necessary prefixes or options tailored to the model being tested. The user is instructed to execute this script multiple times to gather sufficient data. After collecting the results, the user runs `graph.py` to generate a visual representation of the model comparisons. The note emphasizes that the benchmark for human performance is based on a specific set of 100 questions, and altering these questions will invalidate the comparison.
