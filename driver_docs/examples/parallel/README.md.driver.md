# Purpose
The provided content is a documentation snippet for a simulation example within the "llama.cpp" project, specifically focusing on handling parallel client requests. It describes how to simulate 128 client requests with 8 concurrent clients using a command-line tool called `llama-parallel`. The example demonstrates the configuration of the simulation, including parameters like the number of requests, concurrent clients, and the use of a shared system prompt. Additionally, it advises using base models for this simulation, as instruction-tuned models may not adhere to the custom chat template, potentially leading to unexpected results. This documentation is intended to guide users in setting up and understanding the parallel request simulation process.
