
## Files
- **[CMakeLists.txt](src/CMakeLists.txt.driver.md)**: The `CMakeLists.txt` file in the `llama.cpp/src` directory configures the build system for the `llama` library, specifying source files, include directories, compile features, and linking with the `ggml` library.
- **[llama-adapter.cpp](src/llama-adapter.cpp.driver.md)**: The `llama-adapter.cpp` file in the `llama.cpp` codebase implements functionality for initializing and applying control vectors and LoRA adapters to a model, including tensor management and error handling.
- **[llama-adapter.h](src/llama-adapter.h.driver.md)**: The `llama-adapter.h` file defines structures and functions for adapting LLaMA models using context vectors and LoRA weights, including methods for tensor manipulation and scaling.
- **[llama-arch.cpp](src/llama-arch.cpp.driver.md)**: The `llama-arch.cpp` file in the `llama.cpp` codebase defines mappings and functions for handling various large language model architectures and their associated tensor operations and properties.
- **[llama-arch.h](src/llama-arch.h.driver.md)**: The `llama-arch.h` file in the `llama.cpp` codebase defines enumerations and structures for handling various large language model architectures, key-value pairs, and tensor information, along with utility functions for naming and retrieving architecture and tensor details.
- **[llama-batch.cpp](src/llama-batch.cpp.driver.md)**: The `llama-batch.cpp` file in the `llama.cpp` codebase provides functionality for managing and manipulating batches of tokens, including reserving, splitting, and initializing batches with support for embeddings and sequence identifiers.
- **[llama-batch.h](src/llama-batch.h.driver.md)**: The `llama-batch.h` file in the `llama.cpp` codebase defines structures and functions for handling sequence-length-aware batch processing, including metadata management and batch splitting strategies.
- **[llama-chat.cpp](src/llama-chat.cpp.driver.md)**: The `llama-chat.cpp` file in the `llama.cpp` codebase provides functionality for detecting and applying various chat templates to chat messages, supporting a wide range of predefined templates for different chat models.
- **[llama-chat.h](src/llama-chat.h.driver.md)**: The `llama-chat.h` file in the `llama.cpp` codebase defines an enumeration of chat templates and declares functions for handling these templates, including conversion from strings, detection, and application to chat messages.
- **[llama-context.cpp](src/llama-context.cpp.driver.md)**: The `llama-context.cpp` file in the `llama.cpp` codebase implements the `llama_context` class, which manages the context for a LLaMA model, including initialization, encoding, decoding, memory management, state handling, and performance tracking.
- **[llama-context.h](src/llama-context.h.driver.md)**: The `llama-context.h` file in the `llama.cpp` codebase defines the `llama_context` class, which manages the initialization, synchronization, and processing of computation graphs, memory states, and training operations for a machine learning model.
- **[llama-cparams.cpp](src/llama-cparams.cpp.driver.md)**: The `llama-cparams.cpp` file defines a function that returns the maximum number of parallel sequences allowed, as specified by `LLAMA_MAX_PARALLEL_SEQUENCES`.
- **[llama-cparams.h](src/llama-cparams.h.driver.md)**: The `llama-cparams.h` file defines a structure for configuring parameters related to inference and processing in the `llama.cpp` codebase, including context size, threading, and various hyperparameters.
- **[llama-grammar.cpp](src/llama-grammar.cpp.driver.md)**: The `llama-grammar.cpp` file in the `llama.cpp` codebase provides a comprehensive implementation for parsing and managing grammar rules, including UTF-8 decoding, rule parsing, and grammar stack manipulation, to support grammar-based token processing.
- **[llama-grammar.h](src/llama-grammar.h.driver.md)**: The `llama-grammar.h` file defines structures, enumerations, and functions for handling grammar rules and parsing within the `llama.cpp` codebase, including support for lazy grammars and trigger patterns.
- **[llama-graph.cpp](src/llama-graph.cpp.driver.md)**: The `llama-graph.cpp` file in the `llama.cpp` codebase implements various functions and classes for setting up and managing input embeddings, attention mechanisms, and other components of a neural network graph, specifically tailored for the LLaMA model architecture.
- **[llama-graph.h](src/llama-graph.h.driver.md)**: The `llama-graph.h` file in the `llama.cpp` codebase defines various classes and structures for building and managing graph inputs, results, and contexts for multi-modal models, including support for different graph types, input handling, and attention mechanisms.
- **[llama-hparams.cpp](src/llama-hparams.cpp.driver.md)**: The `llama-hparams.cpp` file in the `llama.cpp` codebase defines various methods for managing and retrieving hyperparameters related to layers, heads, and embeddings, including support for stochastic weight averaging (SWA) patterns.
- **[llama-hparams.h](src/llama-hparams.h.driver.md)**: The `llama-hparams.h` file defines the hyperparameters and related configurations for the LLaMA model, including settings for layers, experts, attention mechanisms, and various scaling factors.
- **[llama-impl.cpp](src/llama-impl.cpp.driver.md)**: The `llama-impl.cpp` file in the `llama.cpp` codebase provides logging functionality, string formatting utilities, and functions for handling tensor shapes and GGUF data types.
- **[llama-impl.h](src/llama-impl.h.driver.md)**: The `llama-impl.h` file in the `llama.cpp` codebase provides logging utilities, helper structures, and functions for formatting and manipulating strings and tensor shapes.
- **[llama-io.cpp](src/llama-io.cpp.driver.md)**: The `llama-io.cpp` file implements functions for writing and reading strings with size information in the `llama.cpp` codebase.
- **[llama-io.h](src/llama-io.h.driver.md)**: The `llama-io.h` file defines interfaces for reading and writing operations, including methods for handling raw data and tensors, within the `llama.cpp` codebase.
- **[llama-kv-cache-recurrent.cpp](src/llama-kv-cache-recurrent.cpp.driver.md)**: The `llama-kv-cache-recurrent.cpp` file in the `llama.cpp` codebase implements a recurrent key-value cache system for managing sequence data in neural network models, including functions for initializing, updating, and manipulating cache states.
- **[llama-kv-cache-recurrent.h](src/llama-kv-cache-recurrent.h.driver.md)**: The `llama-kv-cache-recurrent.h` file defines the `llama_kv_cache_recurrent` class, which manages key-value cache states for recurrent graph computations in the `llama.cpp` codebase, along with its associated state class `llama_kv_cache_recurrent_state`.
- **[llama-kv-cache-unified-iswa.cpp](src/llama-kv-cache-unified-iswa.cpp.driver.md)**: The `llama-kv-cache-unified-iswa.cpp` file implements a unified key-value cache system with support for both standard and SWA (Stochastic Weight Averaging) caches, providing functionalities for sequence management and memory state handling in the `llama.cpp` codebase.
- **[llama-kv-cache-unified-iswa.h](src/llama-kv-cache-unified-iswa.h.driver.md)**: The `llama-kv-cache-unified-iswa.h` file defines the `llama_kv_cache_unified_iswa` class, which manages key-value caches for both non-SWA and SWA layers of a model, and the `llama_kv_cache_unified_iswa_state` class for handling the state of these caches.
- **[llama-kv-cache-unified.cpp](src/llama-kv-cache-unified.cpp.driver.md)**: The `llama-kv-cache-unified.cpp` file implements a unified key-value cache system for the Llama model, providing functionalities for cache initialization, management, and operations such as adding, removing, and updating sequences, as well as handling defragmentation and state persistence.
- **[llama-kv-cache-unified.h](src/llama-kv-cache-unified.h.driver.md)**: The `llama-kv-cache-unified.h` file defines the `llama_kv_cache_unified` class and its associated state management, providing functionality for managing key-value caches in the `llama.cpp` codebase, including methods for cache initialization, updates, and state read/write operations.
- **[llama-kv-cells.h](src/llama-kv-cells.h.driver.md)**: The `llama-kv-cells.h` file defines the `llama_kv_cells_unified` class, which manages metadata for key-value cells that can be part of multiple sequences simultaneously, including operations for resetting, resizing, and manipulating cell states and positions.
- **[llama-memory.cpp](src/llama-memory.cpp.driver.md)**: The `llama-memory.cpp` file defines a function to combine two memory status values, determining the overall status based on specific conditions.
- **[llama-memory.h](src/llama-memory.h.driver.md)**: The `llama-memory.h` file defines interfaces and structures for managing memory states and operations in batch processing within the `llama.cpp` codebase, including handling key-value caches and memory updates.
- **[llama-mmap.cpp](src/llama-mmap.cpp.driver.md)**: The `llama-mmap.cpp` file in the `llama.cpp` codebase provides implementations for file handling, memory mapping, and memory locking functionalities across different operating systems, including Windows and POSIX-compliant systems.
- **[llama-mmap.h](src/llama-mmap.h.driver.md)**: The `llama-mmap.h` file in the `llama.cpp` codebase defines structures and functions for file handling, memory mapping, and memory locking, including `llama_file`, `llama_mmap`, and `llama_mlock`, with support for operations like reading, writing, and managing memory.
- **[llama-model-loader.cpp](src/llama-model-loader.cpp.driver.md)**: The `llama-model-loader.cpp` file in the `llama.cpp` codebase provides functionality for loading and managing model data, including handling file versions, tensor types, and metadata, as well as supporting operations like memory mapping and tensor validation.
- **[llama-model-loader.h](src/llama-model-loader.h.driver.md)**: The `llama-model-loader.h` file defines the `llama_model_loader` class, which is responsible for loading model weights and managing tensor data for the Llama model, including support for memory mapping and various file versions.
- **[llama-model-saver.cpp](src/llama-model-saver.cpp.driver.md)**: The `llama-model-saver.cpp` file in the `llama.cpp` codebase provides functionality for saving a LLaMA model's parameters and tensors to a file using the GGUF format.
- **[llama-model-saver.h](src/llama-model-saver.h.driver.md)**: The `llama-model-saver.h` file defines the `llama_model_saver` struct, which provides functionality for saving a llama model by adding key-value pairs and tensors, and ultimately saving the model to a specified path.
- **[llama-model.cpp](src/llama-model.cpp.driver.md)**: The `llama-model.cpp` file in the `llama.cpp` codebase is a comprehensive C++ library component designed for managing and configuring various types of language models, such as LLaMA, GPT, and BERT, by providing functionality for loading, configuring, and operating models, including the setup of layers like attention mechanisms and feed-forward networks, and ensuring compatibility with different hardware backends.
- **[llama-model.h](src/llama-model.h.driver.md)**: The `llama-model.h` file in the `llama.cpp` codebase defines the structure and functionality of a LLaMA model, including various model types, layers, and methods for loading and managing model parameters and tensors.
- **[llama-quant.cpp](src/llama-quant.cpp.driver.md)**: The `llama-quant.cpp` file in the `llama.cpp` codebase implements functions for quantizing tensors in a machine learning model, including handling different quantization types, managing threading for performance, and ensuring data integrity during the quantization process.
- **[llama-quant.h](src/llama-quant.h.driver.md)**: The `llama-quant.h` file in the `llama.cpp` codebase is a header file that uses a preprocessor directive to ensure it is included only once in a compilation.
- **[llama-sampling.cpp](src/llama-sampling.cpp.driver.md)**: The `llama-sampling.cpp` file in the `llama.cpp` codebase implements various sampling strategies and utilities for token selection in language models, including methods for initializing, applying, and managing different types of samplers such as greedy, top-k, top-p, and others.
- **[llama-sampling.h](src/llama-sampling.h.driver.md)**: The `llama-sampling.h` file defines structures and functions related to the initialization and management of a sampler chain for the llama project, including parameters for dry testing.
- **[llama-vocab.cpp](src/llama-vocab.cpp.driver.md)**: The `llama-vocab.cpp` file in the `llama.cpp` codebase implements various tokenization methods and vocabularies for the LLaMA model, including support for different tokenizer types such as SPM, BPE, WPM, UGM, and RWKV, along with handling special tokens and providing functions for tokenization and detokenization processes.
- **[llama-vocab.h](src/llama-vocab.h.driver.md)**: The `llama-vocab.h` file defines the `llama_vocab` class, which provides functionality for loading, managing, and processing vocabulary tokens in the `llama.cpp` codebase.
- **[llama.cpp](src/llama.cpp.driver.md)**: The `llama.cpp` file in the `llama.cpp` codebase provides implementations for loading, saving, and managing machine learning models, including support for various backends and features like memory mapping, GPU offloading, and chat template application.
- **[unicode-data.cpp](src/unicode-data.cpp.driver.md)**: The `unicode-data.cpp` file in the `llama.cpp` codebase is a specialized C++ library for handling Unicode data, focusing on character properties, transformations, and text normalization, with global variables like `unicode_ranges_flags`, `unicode_set_whitespace`, `unicode_map_lowercase`, `unicode_map_uppercase`, and `unicode_ranges_nfd` to support tasks such as categorizing characters, identifying whitespace, and converting between uppercase and lowercase, as well as providing normalization form decomposition mappings.
- **[unicode-data.h](src/unicode-data.h.driver.md)**: The `unicode-data.h` file in the `llama.cpp` codebase defines structures and external variables for handling Unicode data, including normalization, whitespace, and case mapping.
- **[unicode.cpp](src/unicode.cpp.driver.md)**: The `unicode.cpp` file in the `llama.cpp` codebase provides functions for handling Unicode text, including conversion between UTF-8 and Unicode code points, normalization, and custom regex-based text splitting.
- **[unicode.h](src/unicode.h.driver.md)**: The `unicode.h` file in the `llama.cpp` codebase provides functions and structures for handling Unicode code points, including conversion between UTF-8 and code points, normalization, and categorization based on character properties.
