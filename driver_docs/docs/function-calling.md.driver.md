# Purpose
This document appears to be a configuration and usage guide for enabling function calling in a software system that supports OpenAI-style function calling, specifically within the context of the `llama-server` application. It provides detailed instructions on how to configure and utilize different models with native and generic tool call formats, highlighting the support for various models such as Llama, Functionary, Hermes, and others. The document categorizes templates based on their format handlers and provides examples of how to start the server with specific models and templates. It also includes command-line examples for testing the function calling capabilities using an OpenAI-compatible API backend. The relevance of this file to the codebase is significant as it guides users on how to properly configure and leverage the function calling feature, ensuring compatibility and optimal performance across different models and templates.
# Content Summary
The document outlines the configuration and usage of function calling support in the `llama.cpp` project, specifically focusing on OpenAI-style function calling. This feature is integrated into the `llama-server` when initiated with the `--jinja` flag, allowing for enhanced interaction with various models. The document highlights universal support for function calling across multiple models, including Llama, Functionary, Hermes, Qwen, Mistral Nemo, Firefunction, Command R7B, and DeepSeek R1, among others. Each model has specific native tool call formats, and the document provides a comprehensive list of templates and their corresponding format handlers.

The document also discusses the use of generic tool calls, which are employed when a template is not recognized by native format handlers. This approach, while versatile, may be less efficient and consume more tokens compared to native formats. Developers can override templates using the `--chat-template-file` option to optimize performance.

For practical implementation, the document provides shell commands to start the `llama-server` with various models, ensuring the correct templates are used for native support. It also includes instructions for testing function calling via a command-line interface using OpenAI-compatible API backends. The document emphasizes the importance of using appropriate templates and warns against extreme KV quantizations, which can degrade tool-calling performance.

Additionally, the document provides examples of JSON payloads for testing function calls, such as executing Python code or retrieving weather information, demonstrating the integration of function calling within chat interactions. This comprehensive guide is essential for developers looking to implement and optimize function calling in the `llama.cpp` environment.
