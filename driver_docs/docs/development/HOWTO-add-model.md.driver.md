# Purpose
The provided content is a detailed guide for adding a new model architecture to the `llama.cpp` project, which is part of a machine learning framework. This document outlines a specific process for converting models to the GGUF format, defining model architectures, and implementing the GGML graph for inference. It provides a step-by-step approach, including converting models using Python scripts, defining model parameters and tensor layouts in `llama.cpp`, and building the inference graph. The file serves a narrow but crucial function within the codebase, focusing on extending the framework's capabilities to support new model architectures. It is relevant to developers working on integrating new models into the system, ensuring compatibility with existing backends like CUDA, METAL, and CPU, and maintaining consistency with the GGUF specification. The document also includes references to resources and related pull requests, providing a comprehensive guide for developers to follow.
# Content Summary
The document provides a comprehensive guide for developers on how to add a new model architecture to the `llama.cpp` project. The process involves three main steps: converting the model to GGUF format, defining the model architecture in `llama.cpp`, and building the GGML graph implementation.

1. **Convert the Model to GGUF**: This step is executed using a Python script that leverages the `gguf` library. Developers must choose the appropriate conversion script based on the model architecture, such as `convert_hf_to_gguf.py` for general models or `convert_legacy_llama.py` for legacy Llama models. The conversion involves reading the model configuration, tokenizer, and tensor data, and transforming them into GGUF metadata and tensors. Developers need to define a new `Model` subclass with a `Model.register` annotation, specify the GGUF tensor layout in `constants.py`, and map original tensor names to GGUF equivalents in `tensor_mapping.py`. It is crucial to adhere to naming conventions, ensuring tensor names end with `.weight` or `.bias`.

2. **Define the Model Architecture in `llama.cpp`**: This involves specifying model parameters and tensor layouts within the `llama.cpp` file. Developers must define a new `llm_arch`, set up the tensor layout in `LLM_TENSOR_NAMES`, add any unique metadata in `llm_load_hparams`, and create tensors for inference in `llm_load_tensors`. If the model includes a RoPE operation, the rope type should be added in `llama_rope_type`. It is important to note that `ggml` dimensions are typically in reverse order compared to `pytorch`.

3. **Build the GGML Graph Implementation**: This step requires implementing the inference graph for the new model architecture in `llama_build_graph`. Developers are encouraged to review existing implementations like `build_llama`, `build_dbrx`, or `build_bert` for guidance. Some `ggml` backends may not support all operations, and additional backend implementations can be submitted in separate pull requests. Debugging the inference graph can be facilitated using the `llama-eval-callback`.

The document also provides links to the GGUF specification and various resources, including pull requests and discussions related to model support and conversion processes. These resources offer additional context and support for developers working on integrating new model architectures into the `llama.cpp` project.
