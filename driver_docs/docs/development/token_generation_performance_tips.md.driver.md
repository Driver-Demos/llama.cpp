# Purpose
The provided document is a troubleshooting guide for optimizing token generation performance when using the "llama" model, specifically focusing on GPU and CPU utilization. It instructs users on how to verify that the model is running on a GPU with CUDA by ensuring the correct compilation and configuration of environment variables, and by checking diagnostic outputs to confirm GPU offloading. Additionally, it addresses CPU oversaturation issues by advising on the appropriate setting of the `-t N` (threads) parameter to match the number of physical CPU cores, which can prevent performance bottlenecks. The document also includes a benchmark example that demonstrates the impact of different runtime flags on inference speed, providing a practical reference for users to optimize their configurations.
