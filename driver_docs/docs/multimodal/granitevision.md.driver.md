# Purpose
The provided content is a detailed guide for configuring and running the Granite Vision model, which is a composite model involving both visual and language components. This document outlines the steps to download and set up the model, run a script to separate the visual encoder and projector components, and prepare these components for further processing. It involves creating configuration files, such as `config.json`, which define model parameters like architecture, image grid pinpoints, and other model-specific settings. The guide also covers converting these components into GGUF format, a specific file format used for model deployment, and provides instructions for quantizing the language model component. The relevance of this file to a codebase lies in its role as a comprehensive setup and configuration guide, ensuring that the model is correctly prepared and optimized for use in applications that require both visual and language processing capabilities.
# Content Summary
The provided content is a detailed guide for setting up and running the Granite Vision model, which involves several steps to prepare and convert model components for use with the Llama cpp framework. The document outlines the process of downloading the model, configuring environment variables, and executing scripts to prepare the model for further processing.

1. **Model Setup and Environment Configuration**: The initial step involves cloning the Granite Vision model repository from Hugging Face and setting the `GRANITE_MODEL` environment variable to point to the model's directory. This setup is crucial for subsequent operations that rely on this path.

2. **Running Llava Surgery v2**: The guide instructs users to execute the `llava_surgery_v2.py` script, which processes the model to generate two new files, `llava.clip` and `llava.projector`. These files are essential as they contain the split components of the visual encoder and projector. The document provides a Python snippet to verify the integrity of these files by checking that they contain the expected tensors.

3. **Creating the Visual Component GGUF**: Users are guided to create a directory for visual components and copy the generated `llava.clip` and `llava.projector` files into it. A configuration file (`config.json`) is then created, detailing the model's architecture and parameters, such as `image_grid_pinpoints`, `hidden_size`, and `num_attention_heads`. The guide emphasizes the importance of using correct `image_grid_pinpoints` from the model's configuration. The visual components are then converted to GGUF format using a conversion script, with specific parameters for image mean and standard deviation.

4. **Creating the LLM GGUF**: The document explains how to export the language model (LLM) component of the Granite Vision model using the `transformers` library. It involves setting an export path and using the `AutoTokenizer` and `AutoModelForImageTextToText` classes to save the tokenizer and language model. The exported LLM is then converted to GGUF format using a conversion script.

5. **Quantization**: The guide provides instructions for quantizing the LLM using `llama-quantize`, a process that reduces the model's size and potentially improves performance. However, it notes that the visual encoder cannot be quantized due to the specific tensor dimensions used by the SigLIP visual encoder.

6. **Running the Model in Llama cpp**: Finally, the document describes how to build and run the model using the `llama-mtmd-cli` binary from the Llama cpp project. This step involves passing the GGUF files for both the LLM and visual components to the binary, along with additional parameters for model execution.

Overall, the document provides a comprehensive guide for developers to prepare and run the Granite Vision model, detailing each step with specific commands and configurations necessary for successful execution.
