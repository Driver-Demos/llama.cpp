# Purpose
This file is a configuration and documentation guide for enabling and using multimodal input capabilities in the `llama.cpp` software, which supports both image and audio inputs. It provides specific instructions on how to configure the software to handle multimodal data using the `libmtmd` library, detailing the use of command-line options and specifying models that support these features. The file outlines two primary tools, `llama-mtmd-cli` and `llama-server`, that facilitate this functionality, and it includes examples of how to execute these tools with various options. Additionally, it lists pre-quantized models available for use, categorized by their modality support, such as vision, audio, and mixed modalities, and provides links to their sources on Hugging Face. This documentation is crucial for developers and users who need to configure and optimize the software for multimodal data processing within their applications.
# Content Summary
The provided content is a configuration guide for enabling and utilizing multimodal input capabilities in the `llama.cpp` software, which supports both image and audio inputs through the `libmtmd` library. This functionality is accessible via two tools: `llama-mtmd-cli` and `llama-server`, the latter of which supports an OpenAI-compatible `/chat/completions` API. The document outlines the methods to enable multimodal input, emphasizing the use of the `-hf` option with supported models and the `-m` option for specifying text and multimodal projectors. It also provides instructions for disabling GPU offloading of the multimodal projector.

The guide includes examples of command-line usage for both CLI and server contexts, demonstrating how to load models and manage multimodal settings. It highlights the availability of pre-quantized models, which are ready-to-use and primarily feature `Q4_K_M` quantization. These models are hosted on the Hugging Face platform under the `ggml-org` collection.

The document categorizes models into vision, audio, and mixed modalities, listing specific models under each category. Vision models include various versions of Gemma, SmolVLM, Pixtral, Qwen, Mistral, InternVL, Llama, and Moondream. Audio models include Ultravox and Qwen2-Audio, with a note on the lack of pre-quantized GGUF models for some due to poor results. Mixed modality models, such as Qwen2.5 Omni, support both audio and vision inputs.

Overall, this guide is essential for developers looking to implement or experiment with multimodal capabilities in `llama.cpp`, providing detailed instructions on model usage, configuration options, and available resources.
